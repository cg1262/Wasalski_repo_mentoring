{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC ## Apify Actors Data Pipeline - PySpark Implementation\n",
    "# MAGIC Production-ready implementation with error handling, logging, and schema validation\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Configuration and imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, get_json_object, to_timestamp, \n",
    "    unix_timestamp, round as spark_round, \n",
    "    concat, lpad, lit, collect_list, array_distinct,\n",
    "    udf, struct, from_json\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, TimestampType, LongType, MapType,\n",
    "    ArrayType\n",
    ")\n",
    "import requests\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import json\n",
    "from functools import wraps\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Access secret from Databricks secrets\n",
    "# In production, use: dbutils.secrets.get(scope=\"your-scope\", key=\"apify-api-key\")\n",
    "# apify_secret = dbutils.secrets.get(scope=\"api-keys\", key=\"apify-api-key\") if 'dbutils' in dir() else ''\n",
    "apify_secret = apify_secret\n",
    "# COMMAND ----------\n",
    "\n",
    "class ApifyActorsPySpark:\n",
    "    \"\"\"\n",
    "    PySpark implementation of Apify Actors data pipeline.\n",
    "    Optimized for distributed processing and Databricks runtime.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define schemas for data validation\n",
    "    ACTORS_SCHEMA = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"userId\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"username\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "\n",
    "\n",
    "        StructField(\"raw_stats\", MapType(StringType(), StringType()), True),\n",
    "\n",
    "        StructField(\"total_runs\", IntegerType(), True),\n",
    "\n",
    "        StructField(\"last_run_started_at\", TimestampType(), True),       \n",
    "        StructField(\"createdAt\", TimestampType(), True),\n",
    "        StructField(\"modifiedAt\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    RUNS_SCHEMA = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"userId\", StringType(), False),\n",
    "        StructField(\"actId\", StringType(), True),\n",
    "        StructField(\"actorTaskId\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"buildId\", StringType(), True),\n",
    "        StructField(\"buildNumber\", StringType(), True),\\\n",
    "        StructField(\"defaultDatasetId\", StringType(), True),\\\n",
    "        StructField(\"defaultRequestQueueId\", StringType(), True),\\                \n",
    "\n",
    "        StructField(\"buildNumberInt\", StringType(), True),\n",
    "\n",
    "        StructField(\"meta\", MapType(StringType(), StringType()), True),\n",
    "\n",
    "        StructField(\"startedAt\", StringType(), True),\n",
    "        StructField(\"finishedAt\", StringType(), True)\n",
    "\n",
    "        StructField(\"usageTotalUsd\", DoubleType(), True),\n",
    "    ])\n",
    "    \n",
    "    def __init__(self, spark: SparkSession = None):\n",
    "        \"\"\"\n",
    "        Initialize ApifyActorsPySpark with Spark session and configuration.\n",
    "        \n",
    "        Args:\n",
    "            spark: SparkSession instance (uses global spark if not provided)\n",
    "        \"\"\"\n",
    "        self.spark = spark or globals().get('spark')\n",
    "        if not self.spark:\n",
    "            raise ValueError(\"No Spark session found. Please provide spark session or ensure it's available globally\")\n",
    "        \n",
    "        self.apify_secret = apify_secret\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.base_url = \"https://api.apify.com/v2\"\n",
    "        \n",
    "        # Validate API key\n",
    "        if not self.apify_secret:\n",
    "            self.logger.error(\"Apify API key not configured\")\n",
    "            raise ValueError(\"Apify API key is required\")\n",
    "        \n",
    "        self.logger.info(\"ApifyActorsPySpark initialized successfully\")\n",
    "    \n",
    "    def _make_api_request(self, endpoint: str, max_retries: int = 3) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Make API request with retry logic and error handling.\n",
    "        \n",
    "        Args:\n",
    "            endpoint: API endpoint to call\n",
    "            max_retries: Maximum number of retry attempts\n",
    "            \n",
    "        Returns:\n",
    "            Response data as dictionary or None if failed\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/{endpoint}\"\n",
    "        headers = {\n",
    "            'Accept': 'application/json',\n",
    "            'Authorization': f'Bearer {self.apify_secret}'\n",
    "        }\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                return response.json()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.logger.warning(f\"API request failed (attempt {attempt + 1}/{max_retries}): {str(e)}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    self.logger.error(f\"Failed to fetch data from {endpoint} after {max_retries} attempts\")\n",
    "                    raise\n",
    "        return None\n",
    "    \n",
    "    def get_actors(self):\n",
    "        \"\"\"\n",
    "        Fetch and process actors data using PySpark.\n",
    "        \n",
    "        Original pandas implementation:\n",
    "        # actors = pd.DataFrame(items)\n",
    "        # actors['total_runs'] = actors['stats'].apply(lambda x: x.get('totalRuns') if isinstance(x, dict) else None)\n",
    "        # actors['last_run_started_at'] = actors['stats'].apply(lambda x: x.get('lastRunStartedAt') if isinstance(x, dict) else None)\n",
    "        \n",
    "        Returns:\n",
    "            PySpark DataFrame with actors data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Fetching actors data...\")\n",
    "            \n",
    "            # Fetch data from API\n",
    "            response_data = self._make_api_request(\"acts\")\n",
    "            items = response_data.get(\"data\", {}).get(\"items\", [])\n",
    "            \n",
    "            if not items:\n",
    "                self.logger.warning(\"No actors data returned from API\")\n",
    "                # Return empty DataFrame with schema\n",
    "                return self.spark.createDataFrame([], self.ACTORS_SCHEMA)\n",
    "            \n",
    "            # Create PySpark DataFrame with schema validation\n",
    "            actors_df = self.spark.createDataFrame(items, self.ACTORS_SCHEMA)\n",
    "            \n",
    "            # Cache DataFrame since we'll perform multiple operations\n",
    "            actors_df = actors_df.cache()\n",
    "            \n",
    "            # Extract nested stats fields using Spark SQL functions\n",
    "            # This replaces pandas apply() with native Spark operations\n",
    "            actors_df = actors_df \\\n",
    "                .withColumn(\"total_runs\", \n",
    "                    when(col(\"stats\").isNotNull(), \n",
    "                         col(\"stats\").getItem(\"totalRuns\").cast(IntegerType()))\n",
    "                    .otherwise(None)) \\\n",
    "                .withColumn(\"last_run_started_at\",\n",
    "                    when(col(\"stats\").isNotNull(),\n",
    "                         col(\"stats\").getItem(\"lastRunStartedAt\"))\n",
    "                    .otherwise(None)) \\\n",
    "                .withColumnRenamed(\"stats\", \"raw_stats\")\n",
    "            \n",
    "            self.logger.info(f\"Successfully processed {actors_df.count()} actors\")\n",
    "            \n",
    "            return actors_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fetching actors: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_actor_runs(self):\n",
    "        \"\"\"\n",
    "        Fetch actor runs data.\n",
    "        \n",
    "        Original pandas implementation:\n",
    "        # return pd.DataFrame(response.json().get('data', {}).get('items', []))\n",
    "        \n",
    "        Returns:\n",
    "            PySpark DataFrame with actor runs\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Fetching actor runs data...\")\n",
    "            \n",
    "            response_data = self._make_api_request(\"actor-runs\")\n",
    "            items = response_data.get('data', {}).get('items', [])\n",
    "            \n",
    "            if not items:\n",
    "                self.logger.warning(\"No actor runs data returned from API\")\n",
    "                return self.spark.createDataFrame([], self.RUNS_SCHEMA)\n",
    "            \n",
    "            # Create DataFrame with schema validation\n",
    "            runs_df = self.spark.createDataFrame(items, self.RUNS_SCHEMA)\n",
    "            \n",
    "            self.logger.info(f\"Successfully fetched {runs_df.count()} actor runs\")\n",
    "            \n",
    "            return runs_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fetching actor runs: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_unique_actor_runs(self):\n",
    "        \"\"\"\n",
    "        Get unique actor run IDs.\n",
    "        \n",
    "        Original pandas implementation:\n",
    "        # return self.get_actor_runs()['id'].unique().tolist()\n",
    "        \n",
    "        Returns:\n",
    "            List of unique run IDs\n",
    "        \"\"\"\n",
    "        try:\n",
    "            runs_df = self.get_actor_runs()\n",
    "            \n",
    "            # Use Spark's distinct() instead of pandas unique()\n",
    "            unique_ids = runs_df.select(\"id\").distinct().collect()\n",
    "            \n",
    "            # Convert to list (collect returns Row objects)\n",
    "            return [row.id for row in unique_ids]\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting unique actor runs: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_run(self):\n",
    "        \"\"\"\n",
    "        Fetch detailed run information for all unique runs.\n",
    "        \n",
    "        Original pandas implementation:\n",
    "        # runs['startedAt'] = pd.to_datetime(runs['startedAt'])\n",
    "        # runs['time_to_load_in_sec'] = (runs['finishedAt'] - runs['startedAt']).dt.total_seconds()\n",
    "        # runs['pages_scraped'] = runs['usage'].apply(lambda x: x.get('DATASET_WRITES') if isinstance(x, dict) else None)\n",
    "        \n",
    "        Returns:\n",
    "            PySpark DataFrame with enriched run details\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Fetching detailed run information...\")\n",
    "            \n",
    "            runs_list = []\n",
    "            unique_runs = self.get_unique_actor_runs()\n",
    "            \n",
    "            if not unique_runs:\n",
    "                self.logger.warning(\"No unique runs found\")\n",
    "                return self.spark.createDataFrame([], self.RUNS_SCHEMA)\n",
    "            \n",
    "            # Fetch details for each run (consider batching for large datasets)\n",
    "            for run_id in unique_runs:\n",
    "                try:\n",
    "                    response_data = self._make_api_request(f\"actor-runs/{run_id}\")\n",
    "                    data = response_data.get('data', {})\n",
    "                    if data:\n",
    "                        runs_list.append(data)\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Failed to fetch run {run_id}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if not runs_list:\n",
    "                self.logger.warning(\"No run details fetched\")\n",
    "                return self.spark.createDataFrame([], self.RUNS_SCHEMA)\n",
    "            \n",
    "            # Create DataFrame\n",
    "            runs_df = self.spark.createDataFrame(runs_list)\n",
    "            \n",
    "            # Cache for multiple operations\n",
    "            runs_df = runs_df.cache()\n",
    "            \n",
    "            # Convert timestamps using PySpark functions\n",
    "            runs_df = runs_df \\\n",
    "                .withColumn(\"startedAt\", to_timestamp(col(\"startedAt\"))) \\\n",
    "                .withColumn(\"finishedAt\", to_timestamp(col(\"finishedAt\")))\n",
    "            \n",
    "            # Calculate time metrics using Spark SQL functions\n",
    "            runs_df = runs_df \\\n",
    "                .withColumn(\"time_to_load_in_sec\", \n",
    "                    unix_timestamp(col(\"finishedAt\")) - unix_timestamp(col(\"startedAt\"))) \\\n",
    "                .withColumn(\"time_to_load_in_min\", \n",
    "                    col(\"time_to_load_in_sec\") / 60)\n",
    "            \n",
    "            # Extract pages_scraped from usage map\n",
    "            runs_df = runs_df \\\n",
    "                .withColumn(\"pages_scraped\",\n",
    "                    when(col(\"usage\").isNotNull(),\n",
    "                         col(\"usage\").getItem(\"DATASET_WRITES\").cast(IntegerType()))\n",
    "                    .otherwise(None))\n",
    "            \n",
    "            # Calculate pages per second\n",
    "            runs_df = runs_df \\\n",
    "                .withColumn(\"pages_scraped_per_sec\",\n",
    "                    when(col(\"time_to_load_in_sec\") > 0,\n",
    "                         spark_round(col(\"pages_scraped\") / col(\"time_to_load_in_sec\"), 2))\n",
    "                    .otherwise(None))\n",
    "            \n",
    "            # Create duration formatted string using PySpark functions\n",
    "            # This is more complex than UDF but more performant\n",
    "            runs_df = runs_df \\\n",
    "                .withColumn(\"total_seconds\", (col(\"time_to_load_in_min\") * 60).cast(IntegerType())) \\\n",
    "                .withColumn(\"hours\", (col(\"total_seconds\") / 3600).cast(IntegerType())) \\\n",
    "                .withColumn(\"remaining_seconds\", col(\"total_seconds\") % 3600) \\\n",
    "                .withColumn(\"mins\", (col(\"remaining_seconds\") / 60).cast(IntegerType())) \\\n",
    "                .withColumn(\"secs\", (col(\"remaining_seconds\") % 60).cast(IntegerType())) \\\n",
    "                .withColumn(\"duration\",\n",
    "                    when(col(\"time_to_load_in_min\").isNotNull(),\n",
    "                         concat(\n",
    "                             lpad(col(\"hours\"), 2, \"0\"), lit(\":\"),\n",
    "                             lpad(col(\"mins\"), 2, \"0\"), lit(\":\"),\n",
    "                             lpad(col(\"secs\"), 2, \"0\")\n",
    "                         ))\n",
    "                    .otherwise(None)) \\\n",
    "                .drop(\"total_seconds\", \"hours\", \"remaining_seconds\", \"mins\", \"secs\")\n",
    "            \n",
    "            # Format timestamps as strings\n",
    "            runs_df = runs_df \\\n",
    "                .withColumn(\"startedAt_str\", \n",
    "                    date_format(col(\"startedAt\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                .withColumn(\"finishedAt_str\", \n",
    "                    date_format(col(\"finishedAt\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            \n",
    "            self.logger.info(f\"Successfully processed {runs_df.count()} run details\")\n",
    "            \n",
    "            # Optimize partitioning for downstream operations\n",
    "            runs_df = runs_df.repartition(4, \"actId\")\n",
    "            \n",
    "            return runs_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in get_run: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def save_to_delta(self, df, table_name: str, mode: str = \"overwrite\"):\n",
    "        \"\"\"\n",
    "        Save DataFrame to Delta Lake table.\n",
    "        \n",
    "        Args:\n",
    "            df: PySpark DataFrame to save\n",
    "            table_name: Target Delta table name\n",
    "            mode: Write mode (overwrite, append, etc.)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df.write \\\n",
    "                .mode(mode) \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(f\"apify.{table_name}\")\n",
    "            \n",
    "            self.logger.info(f\"Successfully saved data to apify.{table_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving to Delta table: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Initialize the PySpark implementation\n",
    "api = ApifyActorsPySpark(spark)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Example usage with error handling\n",
    "try:\n",
    "    # Get runs data\n",
    "    runs_df = api.get_run()\n",
    "    display(runs_df.select(\"id\", \"actId\", \"startedAt_str\", \"finishedAt_str\", \n",
    "                           \"duration\", \"pages_scraped\", \"pages_scraped_per_sec\"))\n",
    "    \n",
    "    # Optionally save to Delta Lake\n",
    "    # api.save_to_delta(runs_df, \"actor_runs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Pipeline failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get actor runs\n",
    "try:\n",
    "    actor_runs_df = api.get_actor_runs()\n",
    "    display(actor_runs_df)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to get actor runs: {str(e)}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get actors\n",
    "try:\n",
    "    actors_df = api.get_actors()\n",
    "    display(actors_df)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to get actors: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
